{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","gpuClass":"standard","language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 4: Generating and Finetuning Transformer Language Models With Huggingface ","metadata":{"id":"CeUOYktpXtOz"}},{"cell_type":"markdown","source":"In this project, you will first learn how to use Huggingface's Transformers library to load large language models. Next, we will generate text from these models. Finally, we will finetune models on two tasks (sentiment analysis and machine translation).\n\nThis project will be more open ended than the previous projects. We expect you to learn how to use the huggingface and torch documentation.","metadata":{"id":"35m1Uv47jiNX"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"acPh_4GwYID0"}},{"cell_type":"markdown","source":"First we install and import the required dependencies. These include:\n* `torch` for modeling and training\n* `transformers` for pre-trained models\n* `datasets` from huggingface to load existing datasets.","metadata":{"id":"c7cggO7mjZ5L"}},{"cell_type":"code","source":"%%capture\n!pip install transformers\n!pip install datasets\n!pip install --upgrade sacrebleu sentencepiece\n\n# Standard library imports\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelWithLMHead","metadata":{"id":"4LV8KY6_unfe","execution":{"iopub.status.busy":"2023-04-05T06:02:38.310802Z","iopub.execute_input":"2023-04-05T06:02:38.311504Z","iopub.status.idle":"2023-04-05T06:03:19.813366Z","shell.execute_reply.started":"2023-04-05T06:02:38.311473Z","shell.execute_reply":"2023-04-05T06:03:19.812218Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU.\nWe'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging.","metadata":{"id":"KNT_TURTwIlW"}},{"cell_type":"code","source":"assert torch.cuda.is_available()\ndevice = torch.device(\"cuda\")\nprint(\"Using device:\", device)","metadata":{"id":"KYpIPtqtwVwh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"06558507-5bdf-4af0-c9e3-a8f8ed057ce2","execution":{"iopub.status.busy":"2023-04-05T06:03:23.222575Z","iopub.execute_input":"2023-04-05T06:03:23.223795Z","iopub.status.idle":"2023-04-05T06:03:23.289887Z","shell.execute_reply.started":"2023-04-05T06:03:23.223735Z","shell.execute_reply":"2023-04-05T06:03:23.288728Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading Model","metadata":{"id":"ANK-5cMtYSyH"}},{"cell_type":"markdown","source":"We will use GPT-2 medium for this project. This includes both the GPT-2 tokenizer and the GPT-2 model weights itself. If you want to learn more about this model, you can read the GPT-2 paper https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\n\nLet's first load the tokenizer for the GPT-2 medium model. You can find how to do this by reading the documentation for AutoTokenzier in transformers, and finding the GPT-2 model of ~345 million params in there.","metadata":{"id":"We4sTUA5j0Ab"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# Your code here\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")","metadata":{"id":"Gr9PbArCvCT0","execution":{"iopub.status.busy":"2023-04-05T06:03:23.840585Z","iopub.execute_input":"2023-04-05T06:03:23.840986Z","iopub.status.idle":"2023-04-05T06:03:29.790071Z","shell.execute_reply.started":"2023-04-05T06:03:23.840950Z","shell.execute_reply":"2023-04-05T06:03:29.789016Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeef00e5fbd142af97d9f17ceca77601"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23a39770f03143d79ce0bfb628673ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc4328975b5d41a696ba08400d855728"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f995732174c1433e922d7d478c0518e7"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's tokenize and detokenize some text from this model.","metadata":{"id":"V0rkjeg5bezU"}},{"cell_type":"code","source":"print(tokenizer.encode('Hello world'))\nprint(tokenizer.decode(tokenizer.encode('Hello world')))\nprint(tokenizer.encode(\"Hola, c√≥mo est√°süòç\"))","metadata":{"id":"HALePz7Cbjzj","execution":{"iopub.status.busy":"2023-04-05T06:03:29.791880Z","iopub.execute_input":"2023-04-05T06:03:29.792265Z","iopub.status.idle":"2023-04-05T06:03:29.806710Z","shell.execute_reply.started":"2023-04-05T06:03:29.792233Z","shell.execute_reply":"2023-04-05T06:03:29.805676Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[15496, 995]\nHello world\n[39, 5708, 11, 269, 10205, 5908, 1556, 40138, 47249, 235]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's load the GPT-2 medium model. Make sure you also put the model onto the GPU.","metadata":{"id":"v4aSFr-hmKpw"}},{"cell_type":"code","source":"from transformers import AutoModelWithLMHead\n# Your code here\ngpt2_model = AutoModelWithLMHead.from_pretrained(\"gpt2-medium\").to(device)","metadata":{"id":"W1owrfI1xjby","execution":{"iopub.status.busy":"2023-04-05T06:03:29.807921Z","iopub.execute_input":"2023-04-05T06:03:29.808224Z","iopub.status.idle":"2023-04-05T06:03:47.374032Z","shell.execute_reply.started":"2023-04-05T06:03:29.808196Z","shell.execute_reply":"2023-04-05T06:03:47.372966Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:1252: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7cfc069e0cc4aaf8dfad6876b940994"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07fee90a4f034daeb680ef1a752dc217"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Generate From the Model","metadata":{"id":"k1aBbR3snOjV"}},{"cell_type":"markdown","source":"Now let's generate some text from the model to test its LM capabilities. Let's generate 10 pieces of random text of length 50 tokens from the model using random sampling with temperature set to 0.7. This will allow the text to be somewhat high in diversity (random sampling) while maintaining reasonable quality (temperature < 1). When generating text, you can condition on phrases such as \"The coolest thing in NLP right now is\". Find the relevant function and arguments to use for generating text using the Huggingface documentation.\n\nHint: you may find https://huggingface.co/docs/transformers/main_classes/text_generation to be useful for learning about generating from LMs.","metadata":{"id":"5Mo0tnIbnQz8"}},{"cell_type":"code","source":"inputs = tokenizer(\"<|startoftext|>The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n# Your code here\nsample_outputs = gpt2_model.generate(inputs, do_sample=True, temperature=0.7, top_k=2, max_new_tokens=50)","metadata":{"id":"8xSUaso9vo1V","execution":{"iopub.status.busy":"2023-04-05T06:03:47.376564Z","iopub.execute_input":"2023-04-05T06:03:47.376865Z","iopub.status.idle":"2023-04-05T06:03:49.917313Z","shell.execute_reply.started":"2023-04-05T06:03:47.376835Z","shell.execute_reply":"2023-04-05T06:03:49.916259Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Now lets print the text.","metadata":{"id":"7uVROVwndt_z"}},{"cell_type":"code","source":"for i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","metadata":{"id":"koCYZxALdvjX","execution":{"iopub.status.busy":"2023-04-05T06:03:49.919638Z","iopub.execute_input":"2023-04-05T06:03:49.919929Z","iopub.status.idle":"2023-04-05T06:03:49.928258Z","shell.execute_reply.started":"2023-04-05T06:03:49.919902Z","shell.execute_reply":"2023-04-05T06:03:49.927176Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"0: <|startoftext|>The coolest thing right now in NLP is the ability to use the \"word\" as a \"word\" in a sentence, and to use it as a word in a sentence. This is really cool, and I'm really excited about it. I'm also excited about the fact that I\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now generate one piece of text of length 50 with the same prompt (\"The coolest thing right now in NLP is\") but use greedy decoding (temperature = 0). This roughly corresponds to generating some text that is high likelihood for the model.","metadata":{"id":"ox_6NYWCoydJ"}},{"cell_type":"code","source":"inputs = tokenizer(\"<|startoftext|>The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n# Your code here\nsample_outputs = gpt2_model.generate(inputs, max_new_tokens=50)\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","metadata":{"id":"EQEwpPUHePmC","execution":{"iopub.status.busy":"2023-04-05T06:03:49.929864Z","iopub.execute_input":"2023-04-05T06:03:49.930773Z","iopub.status.idle":"2023-04-05T06:03:50.912464Z","shell.execute_reply.started":"2023-04-05T06:03:49.930734Z","shell.execute_reply":"2023-04-05T06:03:50.911374Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0: <|startoftext|>The coolest thing right now in NLP is the ability to use the word \"startoftext\" to refer to a word that is not part of the text. This is useful for things like \"the word\" in a sentence, or \"the word\" in a sentence that is not part\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's try to see how good of a translation system GPT-2 medium is when used \"out of the box\". To accomplish this, we can condition on a prompt like the one below and generate from the model with greedy decoding. This will attempt to translate the sentence \"UC Berkeley ist eine Schule in Kalifornien\", which means \"UC Berkeley is a school in California\". Make sure to set the max length to be high enough so that the model generates sufficient text.","metadata":{"id":"fjVBlj9yfPbx"}},{"cell_type":"code","source":"prompt = \"\"\"Translate the following texts into English.\n\nGerman: UC Berkeley ist eine Schule in Kalifornien\nEnglish:\"\"\"","metadata":{"id":"Xj-OHYlppX4N","execution":{"iopub.status.busy":"2023-04-05T06:03:50.914101Z","iopub.execute_input":"2023-04-05T06:03:50.914482Z","iopub.status.idle":"2023-04-05T06:03:50.922108Z","shell.execute_reply.started":"2023-04-05T06:03:50.914443Z","shell.execute_reply":"2023-04-05T06:03:50.921091Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Your code here. Generate from the model using greedy decoding with the above prompt\ntokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\nsample_outputs = gpt2_model.generate(tokens, max_new_tokens=50)\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","metadata":{"id":"VfqUVa8NfO2j","execution":{"iopub.status.busy":"2023-04-05T06:03:50.923489Z","iopub.execute_input":"2023-04-05T06:03:50.924447Z","iopub.status.idle":"2023-04-05T06:03:52.307521Z","shell.execute_reply.started":"2023-04-05T06:03:50.924418Z","shell.execute_reply":"2023-04-05T06:03:52.306385Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0: Translate the following texts into English.\n\nGerman: UC Berkeley ist eine Schule in Kalifornien\nEnglish: UC Berkeley ist eine Schule in Kalifornien\n\nEnglish: UC Berkeley ist eine Schule in Kalifornien\n\nEnglish: UC Berkeley ist eine Schule in Kalifornien\n\nEnglish\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\\As we can see, translation quality is terrible, as it just repeats the words from the previous text.","metadata":{"id":"TBGuw-18qjsB"}},{"cell_type":"markdown","source":"Now, let's finetune GPT-2 on the translation task to improve the results. We will use a translation dataset from the Huggingface dataset repository (it has thousands of other datasets available). This dataset is one of TED talks translated between German and English.","metadata":{"id":"GlBIoN4aragA"}},{"cell_type":"code","source":"import datasets\ndataset = datasets.load_dataset(\"ted_talks_iwslt\", language_pair=(\"de\", \"en\"), year=\"2014\")","metadata":{"id":"SJYzMQxfvrr0","execution":{"iopub.status.busy":"2023-04-05T06:03:52.311604Z","iopub.execute_input":"2023-04-05T06:03:52.313856Z","iopub.status.idle":"2023-04-05T06:04:49.414387Z","shell.execute_reply.started":"2023-04-05T06:03:52.313816Z","shell.execute_reply":"2023-04-05T06:04:49.413307Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05a7bed9772a436899392735772be0f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52b9664d7ba4080ae4363febc2f8d3b"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset ted_talks_iwslt/de_en_2014 to /root/.cache/huggingface/datasets/ted_talks_iwslt/de_en_2014-c6e771351acd148b/1.1.0/43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20edb36410544791a9c077322ad273c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset ted_talks_iwslt downloaded and prepared to /root/.cache/huggingface/datasets/ted_talks_iwslt/de_en_2014-c6e771351acd148b/1.1.0/43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45f5a5a764704553aaba4fea60723534"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"Xm-2v9h3mav8","execution":{"iopub.status.busy":"2023-04-05T06:04:49.418511Z","iopub.execute_input":"2023-04-05T06:04:49.419247Z","iopub.status.idle":"2023-04-05T06:04:49.424403Z","shell.execute_reply.started":"2023-04-05T06:04:49.419205Z","shell.execute_reply":"2023-04-05T06:04:49.422857Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(dataset['train'][0]['translation'])","metadata":{"id":"OSvnbe8Bj8f0","execution":{"iopub.status.busy":"2023-04-05T06:04:49.425732Z","iopub.execute_input":"2023-04-05T06:04:49.426793Z","iopub.status.idle":"2023-04-05T06:04:49.438313Z","shell.execute_reply.started":"2023-04-05T06:04:49.426712Z","shell.execute_reply":"2023-04-05T06:04:49.437072Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{'de': '\"Ich habe Zerebralparese. Ich zappele die ganze Zeit\", k√ºndigt Maysoon Zayid zu Anfang dieses ungeheuer witzigen, erheiternden an. (Er ist wirklich ungeheur witzig.) \"Als w√ºrde Shakira auf Muhammad Ali treffen.\" Elegant und scharfsinnig nimmt uns die arabisch-amerikanische Komikerin auf eine Reise durch ihre Abenteuer als Schauspielerin, Komikerin, Philanthropin und F√ºrsprecherin f√ºr Menschen mit Behinderungen mit.', 'en': '\"I have cerebral palsy. I shake all the time,\" Maysoon Zayid announces at the beginning of this exhilarating, hilarious talk. (Really, it\\'s hilarious.) \"I\\'m like Shakira meets Muhammad Ali.\" With grace and wit, the Arab-American comedian takes us on a whistle-stop tour of her adventures as an actress, stand-up comic, philanthropist and advocate for the disabled.'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can create a dataset. For each element in the dataset, it should have a text prompt and then the translation, similar to above. Your job is to fill in the labels field below. This field sets the labels to use for training during the language modeling task. \n\nFor the labels, we only want to train the model to output the text after the words \"English:\". This is because in the prompt, everything before the words \"English:\" will also be provided to the model as input. Hint: use -100 as the label for tokens you do not want to train on.\nHint 2: When doing LM training, the labels are the same as the input tokens, except shifted to the left by one. You should check whether Huggingface is already doing the shifting, or whether you need to do the shifting yourself.\n\nOne thing to be careful of with all LMs is to make sure there are not extra spaces. So, the text should be formatted as like \"English: Hello...\" not \"English:  Hello...\". This issue is a common problem people face when using APIs like GPT-3 which we will cover next time.","metadata":{"id":"ISt5r0qKnGCg"}},{"cell_type":"code","source":"prompt = \"\"\"Translate the following texts into English.\nGerman: \"\"\"\n\nclass TranslationDataset(Dataset):\n    def __init__(self, examples, tokenizer):\n        self.input_ids = []\n        self.attn_masks = []\n        self.labels = []\n        for example in examples:\n            training_text = prompt + example['translation']['de'] + '\\nEnglish: ' + example['translation']['en'] + \"<|endoftext|>\"\n            encodings_dict = tokenizer(training_text, max_length=275, padding=\"max_length\", truncation=True)\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n            prompt_and_input_length = len(tokenizer.encode(prompt + example['translation']['de'] + '\\nEnglish:'))\n            # your code below\n            label = torch.tensor([-100]*prompt_and_input_length + encodings_dict['input_ids'][prompt_and_input_length:])\n            self.labels.append(label)\n\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {'input_ids':self.input_ids[idx], 'attention_mask':self.attn_masks[idx], 'labels':self.labels[idx]}","metadata":{"id":"ZtBrXm2Ym4uy","execution":{"iopub.status.busy":"2023-04-05T06:04:49.439987Z","iopub.execute_input":"2023-04-05T06:04:49.440415Z","iopub.status.idle":"2023-04-05T06:04:49.450662Z","shell.execute_reply.started":"2023-04-05T06:04:49.440374Z","shell.execute_reply":"2023-04-05T06:04:49.449502Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"translation_dataset = TranslationDataset(dataset['train'], tokenizer)","metadata":{"id":"rjfb7hkmrAze","execution":{"iopub.status.busy":"2023-04-05T06:04:49.452430Z","iopub.execute_input":"2023-04-05T06:04:49.452829Z","iopub.status.idle":"2023-04-05T06:04:52.107648Z","shell.execute_reply.started":"2023-04-05T06:04:49.452790Z","shell.execute_reply":"2023-04-05T06:04:52.106556Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Now let's break the dataset into a train and test split.","metadata":{"id":"l0xe93WarEOd"}},{"cell_type":"code","source":"train_size = int(0.9 * len(translation_dataset))\ntrain_dataset, val_dataset = random_split(translation_dataset, [train_size, len(translation_dataset) - train_size])\nprint(len(train_dataset))\nprint(len(val_dataset))","metadata":{"id":"1cV_oz5rpdOe","execution":{"iopub.status.busy":"2023-04-05T06:04:52.111418Z","iopub.execute_input":"2023-04-05T06:04:52.111738Z","iopub.status.idle":"2023-04-05T06:04:52.120844Z","shell.execute_reply.started":"2023-04-05T06:04:52.111708Z","shell.execute_reply":"2023-04-05T06:04:52.119476Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"2674\n298\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_dataset[0])","metadata":{"id":"Ijr5Pn_uqk5e","execution":{"iopub.status.busy":"2023-04-05T06:04:52.122471Z","iopub.execute_input":"2023-04-05T06:04:52.122846Z","iopub.status.idle":"2023-04-05T06:04:52.135326Z","shell.execute_reply.started":"2023-04-05T06:04:52.122809Z","shell.execute_reply":"2023-04-05T06:04:52.134264Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([ 8291, 17660,   262,  1708, 13399,   656,  3594,    13,   198, 16010,\n           25,  9956,    77,   519, 32188, 19136,  7802,  1931,    74,   917,\n          316,  4656, 15942,  2398,   268,   268,  1471,   660,   287,  4587,\n          356,   270,   268,   370,  2120,   851, 42499,   287, 10564,   368,\n          304,   521,  1806,   677,   831, 18132,    89,    85,   419, 22562,\n          329,    67,   861,  1931,  5576,   257,  3046,    11,   304,   259,\n        14853,   444,   287,   384,  7749,   367,  3849, 39891,   299,   321,\n          641, 18687, 27541,  4763,  1976,    84,   302, 32407,    13,   220,\n         6733, 17380,  7123, 41555,    11,   450,   469,  1455,   268,  3318,\n          555,   527,  9116, 11840,    83,    11,   264,   521,   287,   402,\n          891,   993,    81,    11,   356,   346,   264,   494,   302, 14234,\n         1665,   364, 40004,   307,   372,  3900,   268,    13, 11707,   379,\n         1491,    64,   549,   437,   268,   376, 14163,   336,   695,    83,\n         5576,  7802,  4656,  5513,    86,   959, 10045,  1305,   496,    25,\n          370,   494,   479,  9101, 20471,   268,   266,   343,  2853, 41828,\n          301,  2364,  3077, 37595,  4587, 45371, 19187, 11693,   701, 10255,\n         1357,  4643, 17204,   268,    11,  1540,   354,   304,   500,   778,\n        19725, 10396,   293,  6183, 21361,  1976,    84,  5513,  9116,    83,\n         4801,    11,  1035, 25680,   488, 39909, 30830,  2222,   268,    30,\n          198, 15823,    25,  9956,    77, 18539, 19136,  7802, 25409,  7104,\n         4113,   287,   262, 10595,   995,  1377,   475,   287,   428,  3665,\n         1790,  1561,   339, 28804,   514,   284,  3613,   257, 31354,   287,\n          465, 24296,    11,  8342,  3340,    13,   383, 17380,  7123, 41555,\n           11,  6569,   290, 37293,    11,   389,   739,  2372,   780,   484,\n         7808,  5527, 13422, 30959,    13,  2080, 13393,  5205,    11,  7802,\n         7893,   257,  5802,  1808,    25,  1374,   460,   356,  5236,  3592,\n          338,   761,   329, 18017,   351,   262, 14960,   284,  1805,   884,\n        21140, 22775,    30, 50256, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  9956,    77, 18539, 19136,  7802, 25409,  7104,\n         4113,   287,   262, 10595,   995,  1377,   475,   287,   428,  3665,\n         1790,  1561,   339, 28804,   514,   284,  3613,   257, 31354,   287,\n          465, 24296,    11,  8342,  3340,    13,   383, 17380,  7123, 41555,\n           11,  6569,   290, 37293,    11,   389,   739,  2372,   780,   484,\n         7808,  5527, 13422, 30959,    13,  2080, 13393,  5205,    11,  7802,\n         7893,   257,  5802,  1808,    25,  1374,   460,   356,  5236,  3592,\n          338,   761,   329, 18017,   351,   262, 14960,   284,  1805,   884,\n        21140, 22775,    30, 50256, 50256])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can use the Huggingface Trainer to finetune GPT-2 on this dataset. This abstracts away all of the details of training. Setup the training arguments to perform 3 epochs of training on this dataset, use a per-device batch size of 2 with gradient accumulation set to 8, use 100 warmup steps, a weight decay of 0.05. Set the eval batch size to be 2. Save a checkpoint every 250 steps. Set fp16 to True. Save the checkpoint in a specific output_dir so you can load it later. Hint: if it tries to launch Wandb, you may add the argument report_to=\"none\".","metadata":{"id":"1sLfI6vDriWK"}},{"cell_type":"code","source":"# Your code here\ntraining_args = TrainingArguments(\n    output_dir='./data',\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=100,\n    weight_decay=0.05,\n    per_device_eval_batch_size=2,\n    save_steps=1,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    fp16=True,\n    report_to=\"none\"\n)","metadata":{"id":"eC4S6WBCsjOW","execution":{"iopub.status.busy":"2023-04-05T06:04:52.136807Z","iopub.execute_input":"2023-04-05T06:04:52.137258Z","iopub.status.idle":"2023-04-05T06:04:52.146090Z","shell.execute_reply.started":"2023-04-05T06:04:52.137218Z","shell.execute_reply":"2023-04-05T06:04:52.144500Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Next create a Huggingface Trainer object and call train() on it.","metadata":{"id":"qsxRb6TOVHn5"}},{"cell_type":"code","source":"# Your code here\ntrainer = Trainer(\n    gpt2_model,\n    training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\ntrainer.train()","metadata":{"id":"xheKi30BVVJC","execution":{"iopub.status.busy":"2023-04-05T06:04:52.147885Z","iopub.execute_input":"2023-04-05T06:04:52.148283Z","iopub.status.idle":"2023-04-05T06:25:00.980596Z","shell.execute_reply.started":"2023-04-05T06:04:52.148247Z","shell.execute_reply":"2023-04-05T06:25:00.978338Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [501/501 20:06, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.477291</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.441537</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.857900</td>\n      <td>0.440185</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=501, training_loss=0.8569660044001962, metrics={'train_runtime': 1208.7733, 'train_samples_per_second': 6.636, 'train_steps_per_second': 0.414, 'total_flos': 4000487073792000.0, 'train_loss': 0.8569660044001962, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Now load your saved checkpoint and see how well the finetuned GPT-2 model does on translating the sentence from before.","metadata":{"id":"SnbquLGFx0Y2"}},{"cell_type":"code","source":"# Jim and Carey: The secret to being happy\n\nprompt = \"\"\"Translate the following texts into English.\n\nGerman: Jim und Carey: Das Geheimnis des Gl√ºcklichseins\nEnglish:\"\"\"\n\n# your code here\nmodel = AutoModelWithLMHead.from_pretrained('./data/checkpoint-501').to(device)\ntokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\nsample_outputs = model.generate(tokens, max_new_tokens=50)\nprint(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))","metadata":{"id":"aOIL0Ha_Fqk3","execution":{"iopub.status.busy":"2023-04-05T06:51:02.846798Z","iopub.execute_input":"2023-04-05T06:51:02.847208Z","iopub.status.idle":"2023-04-05T06:51:07.511291Z","shell.execute_reply.started":"2023-04-05T06:51:02.847170Z","shell.execute_reply":"2023-04-05T06:51:07.510109Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Translate the following texts into English.\n\nGerman: Jim und Carey: Das Geheimnis des Gl√ºcklichseins\nEnglish: Jim and Carey: The secret to happiness\n","output_type":"stream"}]},{"cell_type":"code","source":"# Arthur Bailey: Can technology solve all of our problems?\n\nprompt = \"\"\"Translate the following texts into English.\n\nGerman: Arthur Bailey: Kann Technologie all unsere Probleme l√∂sen?\nEnglish:\"\"\"\n\n# your code here\nmodel = AutoModelWithLMHead.from_pretrained('./data/checkpoint-501').to(device)\ntokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\nsample_outputs = model.generate(tokens, max_new_tokens=50)\nprint(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))","metadata":{"id":"3IR7l2P_L9ey","execution":{"iopub.status.busy":"2023-04-05T06:51:17.649194Z","iopub.execute_input":"2023-04-05T06:51:17.649880Z","iopub.status.idle":"2023-04-05T06:51:21.759098Z","shell.execute_reply.started":"2023-04-05T06:51:17.649843Z","shell.execute_reply":"2023-04-05T06:51:21.757984Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Translate the following texts into English.\n\nGerman: Arthur Bailey: Kann Technologie all unsere Probleme l√∂sen?\nEnglish: Arthur Bailey: Can technology solve our problems?\n","output_type":"stream"}]},{"cell_type":"code","source":"# After that they sometimes go shopping.\n\nprompt = \"\"\"Translate the following texts into English.\n\nGerman: Danach gehen sie manchmal noch einkaufen\nEnglish:\"\"\"\n\n# your code here\nmodel = AutoModelWithLMHead.from_pretrained('./data/checkpoint-501').to(device)\ntokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\nsample_outputs = model.generate(tokens, max_new_tokens=50)\nprint(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-04-05T06:51:37.678998Z","iopub.execute_input":"2023-04-05T06:51:37.680053Z","iopub.status.idle":"2023-04-05T06:51:41.806156Z","shell.execute_reply.started":"2023-04-05T06:51:37.680011Z","shell.execute_reply":"2023-04-05T06:51:41.805015Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Translate the following texts into English.\n\nGerman: Danach gehen sie manchmal noch einkaufen\nEnglish: Danelle: How to make your own food\n","output_type":"stream"}]},{"cell_type":"code","source":"# I am pleased to meet you.\n\nprompt = \"\"\"Translate the following texts into English.\n\nGerman: Es freut mich, dich kennenzulernen\nEnglish:\"\"\"\n\n# your code here\nmodel = AutoModelWithLMHead.from_pretrained('./data/checkpoint-501').to(device)\ntokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\nsample_outputs = model.generate(tokens, max_new_tokens=50)\nprint(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-04-05T06:51:55.605260Z","iopub.execute_input":"2023-04-05T06:51:55.605638Z","iopub.status.idle":"2023-04-05T06:51:59.822236Z","shell.execute_reply.started":"2023-04-05T06:51:55.605600Z","shell.execute_reply":"2023-04-05T06:51:59.821084Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Translate the following texts into English.\n\nGerman: Es freut mich, dich kennenzulernen\nEnglish: David Deutsch: How to make free love\n","output_type":"stream"}]},{"cell_type":"code","source":"# A group of men are loading cotton onto a truck\n\nprompt = \"\"\"Translate the following texts into English.\n\nGerman: Eine Gruppe von M√§nnern l√§dt Baumwolle auf einen Lastwagen\nEnglish:\"\"\"\n\n# your code here\nmodel = AutoModelWithLMHead.from_pretrained('./data/checkpoint-501').to(device)\ntokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\nsample_outputs = model.generate(tokens, max_new_tokens=50)\nprint(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-04-05T06:56:43.912460Z","iopub.execute_input":"2023-04-05T06:56:43.913414Z","iopub.status.idle":"2023-04-05T06:56:48.448252Z","shell.execute_reply.started":"2023-04-05T06:56:43.913374Z","shell.execute_reply":"2023-04-05T06:56:48.447266Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Translate the following texts into English.\n\nGerman: Eine Gruppe von M√§nnern l√§dt Baumwolle auf einen Lastwagen\nEnglish: A group of young men in Germany build a last-mile bridge\n","output_type":"stream"}]},{"cell_type":"markdown","source":"If training went correctly, you should see a reasonable translation of the sentence, with some errors.\n\nFor the project report, find two sentences where the model succeeds and two sentences where the model fails. Describe what might be causing these types of failures.","metadata":{"id":"EUCG_xSYN2pn"}},{"cell_type":"markdown","source":"Finally, revisit the code from project 2 on using and running the Multi30k dataset. Your goal will be to translate the test set using the GPT-2 model you just finetuned. You will then submit your test predictions as a txt file, where you place your model's prediction for each test example on a separate line. Feel free to copy and paste any code from HW2 that may be useful. Submit the file named as mt_predictions.txt to gradescope.\n\nThe GPT-2 model may not work that well on the Multi30k dataset, because there is a distribution shift where the Multi30k data looks different than the Ted talks data that you finetuned the model on. The takeaway I want people to have is that a general-purpose LM system can be decent at a task like translation, however, if you create a domain-specific model like a LSTM trained specifically on Multi30k, you can outperform the general purpose model.\n\nFor the project report, compare two translations from the GPT-2 versus LSTM model. Which one works better?\n\nHint: One failure mode for GPT-2 is that it may generate fluent sentences that are actually unrelated to the input.","metadata":{"id":"qC_sFmejXEtA"}},{"cell_type":"code","source":"# Your code for generating mt_predictions.txt below\n!pip install torchtext==0.6","metadata":{"id":"hRws9idEXecL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchtext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nextensions = [\".de\", \".en\"]\nsource_field = torchtext.data.Field(tokenize=lambda x: x)\ntarget_field = torchtext.data.Field(tokenize=lambda x: x)\ntraining_data, validation_data, test_data = torchtext.datasets.Multi30k.splits(\n    extensions, [source_field, target_field], root=\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of training examples:\", len(training_data))\nprint(\"Number of validation examples:\", len(validation_data))\nprint(\"Number of test examples:\", len(test_data))\nprint()\n\nfor example in training_data[:10]:\n  print(example.src)\n  print(example.trg)\n  print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm_notebook\n\nprompt = \"\"\"Translate the following texts into English.\nGerman: \"\"\"\ntranslations = []\nfor i in tqdm_notebook(range(len(test_data))):\n    example = test_data[i]\n    src = example.src\n    text = prompt + src + '\\nEnglish:'\n    tokens = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n    sample_outputs = model.generate(tokens, max_new_tokens=50)\n    text_output = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n    translations.append(text_output[len(text)+1:])\ntranslations = '\\n'.join(translations)\nwith open('mt_predictions.txt', 'w') as file:\n    file.write(translations)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-04T05:36:05.760750Z","iopub.execute_input":"2023-04-04T05:36:05.761996Z","iopub.status.idle":"2023-04-04T05:41:20.276773Z","shell.execute_reply.started":"2023-04-04T05:36:05.761946Z","shell.execute_reply":"2023-04-04T05:41:20.275563Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e7705772beb4ba985ee57d75e10973e"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Sentiment Analysis","metadata":{"id":"y0WsHbgnOAFx"}},{"cell_type":"markdown","source":"The beauty of language models is that we can apply this exact same machinery to solve a completely different task of sentiment analysis. Here, we will be given a movie review and the goal is to have the model predict whether the review is positive or negative.","metadata":{"id":"3VmRQfoyOA2G"}},{"cell_type":"markdown","source":"First, we will load some sentiment analysis data. Your job is to copy what we did above for machine translation to load the dataset, build a Class to create the dataset, etc., \n\nWhen doing so, use the prompt below, where you put the text of the input in the first [] and in the second [], put the word Positive if the label is 1 and the word Negative if the label is 0. Make sure to also set the self.labels field correctly, we only want to compute a loss on the words Positive/Negative, and no other tokens in the model's input.\n\nThe following is a movie review. [Movie Review Text Here]. The sentiment of the review is [Positive/Negative].","metadata":{"id":"JhJ5ptLyOoGI"}},{"cell_type":"code","source":"import datasets\ndataset = datasets.load_dataset('glue', 'sst2')\nprint(dataset['train'][0])","metadata":{"id":"Qk67kbfPQAGy","execution":{"iopub.status.busy":"2023-04-04T05:46:20.915710Z","iopub.execute_input":"2023-04-04T05:46:20.916431Z","iopub.status.idle":"2023-04-04T05:46:26.190212Z","shell.execute_reply.started":"2023-04-04T05:46:20.916393Z","shell.execute_reply":"2023-04-04T05:46:26.189116Z"},"trusted":true},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d36eb52ec82f42678d7f2582ee252fcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6244978399e4317abf28f931b1849e9"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"126d23c48a3b4b46970c3afe4a831f64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d076605156c64160b4053f0a1548073f"}},"metadata":{}},{"name":"stdout","text":"{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note: Some people were saying that this line of code wasn't working and they needed to use \"dataset = datasets.load_dataset('glue', 'sst2')\" instead.","metadata":{"id":"tJ0CbUm8L0ZR"}},{"cell_type":"code","source":"prompt = \"\"\"The following is a movie review. \"\"\"\nclass SentimentDataset(Dataset):\n    def __init__(self, examples, tokenizer):\n        self.input_ids = []\n        self.attn_masks = []\n        self.labels = []\n        for example in examples:\n            sentiment = 'Positive' if example['label'] else 'Negative'\n            training_text = prompt + example['sentence'][:-1] + '. The sentiment of the review is ' + sentiment + \".<|endoftext|>\"\n            encodings_dict = tokenizer(training_text, max_length=275, padding=\"max_length\", truncation=True)\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n            prompt_and_input_length = len(tokenizer.encode(prompt + example['sentence'][:-1] + '.The sentiment of the review is'))\n            # your code below\n            label = torch.tensor([-100]*prompt_and_input_length + encodings_dict['input_ids'][prompt_and_input_length:])\n            self.labels.append(label)\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {'input_ids':self.input_ids[idx], 'attention_mask':self.attn_masks[idx], 'labels':self.labels[idx]}","metadata":{"id":"j7_GzIgmRIGC","execution":{"iopub.status.busy":"2023-04-04T05:46:28.162682Z","iopub.execute_input":"2023-04-04T05:46:28.163177Z","iopub.status.idle":"2023-04-04T05:46:28.178042Z","shell.execute_reply.started":"2023-04-04T05:46:28.163141Z","shell.execute_reply":"2023-04-04T05:46:28.175877Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"sentiment_train_dataset = SentimentDataset(dataset['train'], tokenizer)\nsentiment_val_dataset = SentimentDataset(dataset['validation'], tokenizer)","metadata":{"id":"2lsk5uGNRIGE","execution":{"iopub.status.busy":"2023-04-04T05:46:29.534354Z","iopub.execute_input":"2023-04-04T05:46:29.534849Z","iopub.status.idle":"2023-04-04T05:47:20.818308Z","shell.execute_reply.started":"2023-04-04T05:46:29.534787Z","shell.execute_reply":"2023-04-04T05:47:20.817052Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"The data already comes with a validation and train split","metadata":{"id":"SUye-UIRRIGF"}},{"cell_type":"code","source":"print(len(sentiment_train_dataset))\nprint(len(sentiment_val_dataset))\nprint(sentiment_train_dataset[0])","metadata":{"id":"Kb0XeLriRIGF","execution":{"iopub.status.busy":"2023-04-04T05:47:20.820512Z","iopub.execute_input":"2023-04-04T05:47:20.820884Z","iopub.status.idle":"2023-04-04T05:47:20.832627Z","shell.execute_reply.started":"2023-04-04T05:47:20.820846Z","shell.execute_reply":"2023-04-04T05:47:20.831390Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"67349\n872\n{'input_ids': tensor([  464,  1708,   318,   257,  3807,  2423,    13,  7808,   649,  3200,\n          507,   422,   262, 21694,  4991,    13,   383, 15598,   286,   262,\n         2423,   318, 36183,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100, 36183,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's train the model using the same trainer arguments as before, except just do $<$1 epoch of training because this dataset is quite large and training on the entire thing will take some time. Make sure you also use a different output_dir so it doesn't overwrite your old results.","metadata":{"id":"Q7vOS-ZcTHds"}},{"cell_type":"code","source":"# Your code here\ngpt2_model = AutoModelWithLMHead.from_pretrained(\"gpt2-medium\").to(device)\ntraining_args = TrainingArguments(\n    output_dir='./sentiment',\n    num_train_epochs=0.4,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=100,\n    weight_decay=0.05,\n    per_device_eval_batch_size=2,\n    save_steps=1,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    fp16=True,\n    report_to=\"none\"\n)\ntrainer = Trainer(\n    gpt2_model,\n    training_args,\n    train_dataset=sentiment_train_dataset,\n    eval_dataset=sentiment_val_dataset,\n    tokenizer=tokenizer\n)\ntrainer.train()","metadata":{"id":"vEAs8zFDVdY4","execution":{"iopub.status.busy":"2023-04-04T05:48:18.511611Z","iopub.execute_input":"2023-04-04T05:48:18.511992Z","iopub.status.idle":"2023-04-04T06:52:10.095497Z","shell.execute_reply.started":"2023-04-04T05:48:18.511958Z","shell.execute_reply":"2023-04-04T06:52:10.094385Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1684' max='1684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1684/1684 1:03:43, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.000700</td>\n      <td>0.000806</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1684, training_loss=0.1422494201701922, metrics={'train_runtime': 3825.4557, 'train_samples_per_second': 7.042, 'train_steps_per_second': 0.44, 'total_flos': 1.34400403636224e+16, 'train_loss': 0.1422494201701922, 'epoch': 0.4})"},"metadata":{}}]},{"cell_type":"markdown","source":"At test-time, when you want to classify an incoming movie review, you can just check whether the model generates the words Positive or Negative as the final word.","metadata":{"id":"azaXJt4cPV3Y"}},{"cell_type":"code","source":"prompt = \"\"\"The following is a movie review. The acting was great but overall I was left disappointed by the film. The sentiment of the review is\"\"\"","metadata":{"id":"wlTivogUUyFz","execution":{"iopub.status.busy":"2023-04-04T06:58:38.897554Z","iopub.execute_input":"2023-04-04T06:58:38.897945Z","iopub.status.idle":"2023-04-04T06:58:38.902600Z","shell.execute_reply.started":"2023-04-04T06:58:38.897912Z","shell.execute_reply":"2023-04-04T06:58:38.901560Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# Your code here\nmodel = AutoModelWithLMHead.from_pretrained('./sentiment/checkpoint-1684').to(device)\ntokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\nsample_outputs = model.generate(tokens, max_new_tokens=50)\nprint(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))","metadata":{"id":"l3-2HzbHVJ9L","execution":{"iopub.status.busy":"2023-04-04T06:58:39.066086Z","iopub.execute_input":"2023-04-04T06:58:39.066381Z","iopub.status.idle":"2023-04-04T06:58:43.738375Z","shell.execute_reply.started":"2023-04-04T06:58:39.066354Z","shell.execute_reply":"2023-04-04T06:58:43.737409Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"The following is a movie review. The acting was great but overall I was left disappointed by the film. The sentiment of the review is Negative.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Finally, run the entire validation set through the model and get your model predictions. Save the results as a txt file, where each line just contains either \"1\" if your model predicted Positive and \"0\" if the model predicted Negative. You will get full credit if your model's accuracy is greater than 80%. Save the file as sst_predictions.txt and submit it to gradescope.\n\nFor the report, describe two possible improvements to your sentiment classifier.","metadata":{"id":"nc5bgcS7X1BX"}},{"cell_type":"code","source":"# Your code here for generating sst_predictions\nsentiment_val_dataset\nprompt = \"\"\"The following is a movie review. \"\"\"\ntranslations = []\nfor i in tqdm_notebook(range(len(dataset['validation']))):\n    example = dataset['validation'][i]\n    review = example['sentence'][:-1]\n    text = prompt + review + '. The sentiment of the review is'\n    tokens = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n    sample_outputs = model.generate(tokens, max_new_tokens=50)[:, tokens.shape[1]:]\n    text_output = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n    translations.append('1' if text_output[1:-1] == \"Positive\" else '0')\ntranslations = '\\n'.join(translations)\nwith open('sst_predictions.txt', 'w') as file:\n    file.write(translations)","metadata":{"id":"wnmzEl7wYGW2","execution":{"iopub.status.busy":"2023-04-04T07:13:11.742251Z","iopub.execute_input":"2023-04-04T07:13:11.742628Z","iopub.status.idle":"2023-04-04T07:14:06.566667Z","shell.execute_reply.started":"2023-04-04T07:13:11.742586Z","shell.execute_reply":"2023-04-04T07:14:06.565520Z"},"trusted":true},"execution_count":118,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/872 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966ad74b6d59476eb70e53fedd3151ca"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Submission","metadata":{"id":"kotdLszxHWWJ"}},{"cell_type":"markdown","source":"Turn in the following files on Gradescope:\n* hw4.ipynb (this file; please rename to match)\n* mt_predictions.txt (the predictions for the Multi30k test set)\n* sst_predictions.txt (the predictions for the SST-2 validation set)\n* report.pdf\n\nBe sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.","metadata":{"id":"-onu93vgG2-U"}}]}