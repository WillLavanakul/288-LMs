This repo contains projects for Berkeley's graduate NLP course. We implement and cover the following:

- POS Tagging models: neural N-gram models, LSTM networks, ensemble LSTM models
- IBM model 1 for text alignment
- Neural Machine Translation: Seq2Seq models with both attention and vectorized beam search. Additionally examines a visualization of attention
- POS Tagging via Transformers
- Parsing via syntax trees through Transformers
- Finetuned GPT-2 via HF for machine translation and sentiment analysis
- Zero-shot and few-shiot prompting for LLMs using OpenAI api. Also explored diverse prompt selection through cosine similarity and Levenshtein distance
